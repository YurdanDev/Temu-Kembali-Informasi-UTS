{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T06:10:12.851195Z",
     "start_time": "2025-11-10T06:10:12.090197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import urllib\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import threading"
   ],
   "id": "c3b8e31ef0beee56",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T06:10:12.867293Z",
     "start_time": "2025-11-10T06:10:12.857707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "collected = 0\n",
    "pbar = None\n",
    "lock = threading.Lock()\n",
    "\n",
    "def create_path(folder_name):\n",
    "    path = os.path.join(os.getcwd(), folder_name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "def open_page(link):\n",
    "    count = 0\n",
    "    while count < 3:\n",
    "        try:\n",
    "            return BeautifulSoup(requests.get(link).text, \"lxml\")\n",
    "        except:\n",
    "            count += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "def get_detail(soup, keyword):\n",
    "    try:\n",
    "        text = (\n",
    "            soup.find(lambda tag: tag.name == \"td\" and keyword in tag.text)\n",
    "            .find_next()\n",
    "            .get_text()\n",
    "            .strip()\n",
    "        )\n",
    "        return text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_pdf(url, path_pdf):\n",
    "    try:\n",
    "        file = urllib.request.urlopen(url)\n",
    "        file_name = file.info().get_filename().replace(\"/\", \" \")\n",
    "        file_content = file.read()\n",
    "        with open(f\"{path_pdf}/{file_name}\", \"wb\") as out_file:\n",
    "            out_file.write(file_content)\n",
    "        return file_name\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def is_url_already_scraped(url, destination):\n",
    "    \"\"\"\n",
    "    Checks if a URL has already been scraped and saved in the CSV file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to check.\n",
    "        destination (str): The path to the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the URL exists in the CSV, False otherwise.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(f\"{destination}.csv\"):\n",
    "      return False\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(f\"{destination}.csv\")\n",
    "        return url in df[\"link\"].values\n",
    "    except pd.errors.EmptyDataError:\n",
    "      return False"
   ],
   "id": "3300deede714cd31",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T06:10:12.898423Z",
     "start_time": "2025-11-10T06:10:12.877288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_data(link, keyword_url):\n",
    "    global collected\n",
    "    global pbar\n",
    "    global lock\n",
    "    global path_output\n",
    "    global path_pdf\n",
    "\n",
    "    path_output = create_path('data/CSV')\n",
    "    path_pdf = create_path('data/PDF')\n",
    "\n",
    "    keyword_url = keyword_url.replace(\"/\", \" \")\n",
    "    if keyword_url.startswith(\"https\"):\n",
    "        keyword_url = \"\"\n",
    "    destination = f\"{path_output}/putusan_ma_{keyword_url}\"\n",
    "\n",
    "    with lock:\n",
    "        if collected >= 50:\n",
    "            return\n",
    "\n",
    "    if is_url_already_scraped(link, destination):\n",
    "        print(f\"Skipping duplicate URL: {link}\")\n",
    "        logging.info(f\"Skipping duplicate URL: {link}\")\n",
    "        return\n",
    "\n",
    "    soup = open_page(link)\n",
    "    table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "    judul = table.find(\"h2\").text\n",
    "    table.find(\"h2\").decompose()\n",
    "\n",
    "    nomor = get_detail(table, \"Nomor\")\n",
    "    tingkat_proses = get_detail(table, \"Tingkat Proses\")\n",
    "    klasifikasi = get_detail(table, \"Klasifikasi\")\n",
    "    kata_kunci = get_detail(table, \"Kata Kunci\")\n",
    "    tahun = get_detail(table, \"Tahun\")\n",
    "    tanggal_register = get_detail(table, \"Tanggal Register\")\n",
    "    lembaga_peradilan = get_detail(table, \"Lembaga Peradilan\")\n",
    "    jenis_lembaga_peradilan = get_detail(table, \"Jenis Lembaga Peradilan\")\n",
    "    hakim_ketua = get_detail(table, \"Hakim Ketua\")\n",
    "    hakim_anggota = get_detail(table, \"Hakim Anggota\")\n",
    "    panitera = get_detail(table, \"Panitera\")\n",
    "    amar = get_detail(table, \"Amar\")\n",
    "    amar_lainnya = get_detail(table, \"Amar Lainnya\")\n",
    "    catatan_amar = get_detail(table, \"Catatan Amar\")\n",
    "    tanggal_musyawarah = get_detail(table, \"Tanggal Musyawarah\")\n",
    "    tanggal_dibacakan = get_detail(table, \"Tanggal Dibacakan\")\n",
    "    kaidah = get_detail(table, \"Kaidah\")\n",
    "    abstrak = get_detail(table, \"Abstrak\")\n",
    "\n",
    "    link_pdf = \"\"\n",
    "    file_name_pdf = \"\"\n",
    "    try:\n",
    "        link_pdf = soup.find(\"a\", href=re.compile(r\"/pdf/\"))[\"href\"]\n",
    "        file_name_pdf = get_pdf(link_pdf, path_pdf)\n",
    "        if file_name_pdf is None:\n",
    "            raise Exception(\"PDF download failed\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to download PDF for {link}: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    data = [\n",
    "        judul,\n",
    "        nomor,\n",
    "        tingkat_proses,\n",
    "        klasifikasi,\n",
    "        kata_kunci,\n",
    "        tahun,\n",
    "        tanggal_register,\n",
    "        lembaga_peradilan,\n",
    "        jenis_lembaga_peradilan,\n",
    "        hakim_ketua,\n",
    "        hakim_anggota,\n",
    "        panitera,\n",
    "        amar,\n",
    "        amar_lainnya,\n",
    "        catatan_amar,\n",
    "        tanggal_musyawarah,\n",
    "        tanggal_dibacakan,\n",
    "        kaidah,\n",
    "        abstrak,\n",
    "        link,\n",
    "        link_pdf,\n",
    "        file_name_pdf,\n",
    "    ]\n",
    "    result = pd.DataFrame(\n",
    "        [data],\n",
    "        columns=[\n",
    "            \"judul\",\n",
    "            \"nomor\",\n",
    "            \"tingkat_proses\",\n",
    "            \"klasifikasi\",\n",
    "            \"kata_kunci\",\n",
    "            \"tahun\",\n",
    "            \"tanggal_register\",\n",
    "            \"lembaga_peradilan\",\n",
    "            \"jenis_lembaga_peradilan\",\n",
    "            \"hakim_ketua\",\n",
    "            \"hakim_anggota\",\n",
    "            \"panitera\",\n",
    "            \"amar\",\n",
    "            \"amar_lainnya\",\n",
    "            \"catatan_amar\",\n",
    "            \"tanggal_musyawarah\",\n",
    "            \"tanggal_dibacakan\",\n",
    "            \"kaidah\",\n",
    "            \"abstrak\",\n",
    "            \"link\",\n",
    "            \"link_pdf\",\n",
    "            \"file_name_pdf\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(f\"Saving data for {link}\")\n",
    "    if not os.path.isfile(f\"{destination}.csv\"):\n",
    "        result.to_csv(f\"{destination}.csv\", header=True, index=False)\n",
    "    else:\n",
    "        result.to_csv(f\"{destination}.csv\", mode=\"a\", header=False, index=False)\n",
    "\n",
    "    logging.info(f\"Successfully scraped and saved data and PDF for {link} (PDF: {file_name_pdf})\")\n",
    "\n",
    "    with lock:\n",
    "        collected += 1\n",
    "        pbar.update(1)"
   ],
   "id": "852ceaafec9895d0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T06:10:12.929070Z",
     "start_time": "2025-11-10T06:10:12.909411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_scraper(keyword=None, url=None, sort_date=True, download_pdf=True):\n",
    "    global collected\n",
    "    global pbar\n",
    "\n",
    "    path_logs = create_path('data/Logs')\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        filename=os.path.join(path_logs, f\"scraper_log_{date.today().strftime('%Y-%m-%d')}.log\"),\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    if not keyword and not url:\n",
    "        print(\"Please provide a keyword or URL\")\n",
    "        return\n",
    "\n",
    "    path_output = create_path('data/CSV')\n",
    "    path_pdf = create_path('data/PDF')\n",
    "\n",
    "    link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword}&page=1\"\n",
    "\n",
    "    if url:\n",
    "        link = url\n",
    "\n",
    "    soup = open_page(link)\n",
    "\n",
    "    try:\n",
    "        last_page = int(\n",
    "            soup.find_all(\"a\", {\"class\": \"page-link\"})[-1].get(\"data-ci-pagination-page\")\n",
    "        )\n",
    "    except:\n",
    "        print(\"Could not find pagination. Perhaps no results or different structure.\")\n",
    "        return\n",
    "\n",
    "    keyword_url = url if url else keyword\n",
    "\n",
    "    destination = f\"{path_output}/putusan_ma_{keyword_url.replace('/', ' ')}\"\n",
    "\n",
    "    if os.path.isfile(f\"{destination}.csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{destination}.csv\")\n",
    "            collected = len(df)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            collected = 0\n",
    "    else:\n",
    "        collected = 0\n",
    "\n",
    "    pbar = tqdm(total=50, desc=\"Scraping progress\", initial=collected)\n",
    "\n",
    "    if collected >= 50:\n",
    "        print(\"Already reached or exceeded 50 data entries.\")\n",
    "        pbar.close()\n",
    "        return\n",
    "\n",
    "    if url:\n",
    "        print(f\"Scraping with url: {url} - approx {20 * last_page} data - {last_page} pages\")\n",
    "    else:\n",
    "        print(f\"Scraping with keyword: {keyword} - approx {20 * last_page} data - {last_page} pages\")\n",
    "\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for page in range(1, last_page + 1):\n",
    "            futures.append(\n",
    "                executor.submit(run_process, keyword_url, page, sort_date)\n",
    "            )\n",
    "    wait(futures)\n",
    "    pbar.close()"
   ],
   "id": "a30763cc814fbf69",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-10T06:10:12.939948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_process(keyword_url, page, sort_date):\n",
    "    if keyword_url.startswith(\"https\"):\n",
    "        link = keyword_url\n",
    "        if page > 1:\n",
    "            if '?' in link:\n",
    "                link += f\"&page={page}\"\n",
    "            else:\n",
    "                link += f\"?page={page}\"\n",
    "    else:\n",
    "        link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword_url}&page={page}\"\n",
    "    if sort_date:\n",
    "        if '?' in link:\n",
    "            link += \"&obf=TANGGAL_PUTUS&obm=desc\"\n",
    "        else:\n",
    "            link += \"?obf=TANGGAL_PUTUS&obm=desc\"\n",
    "\n",
    "    print(link)\n",
    "    logging.info(f\"Processing page: {link}\")\n",
    "\n",
    "    soup = open_page(link)\n",
    "    links = soup.find_all(\"a\", {\"href\": re.compile(\"/direktori/putusan\")})\n",
    "\n",
    "    for a in links:\n",
    "        extract_data(a[\"href\"], keyword_url)\n",
    "\n",
    "def scrape_specific_url(url, download_pdf=True):\n",
    "    if not url or not url.startswith(\"https://\"):\n",
    "        print(\"Please provide a valid URL\")\n",
    "        return\n",
    "\n",
    "    path_output = create_path('data/CSV')\n",
    "    path_pdf = create_path('data/PDF')\n",
    "\n",
    "    extract_data(url, url)\n",
    "\n",
    "# Putusan PN SURABAYA Narkotika dan Psikotropika Putus Tahun 2025\n",
    "run_scraper(url=\"https://putusan3.mahkamahagung.go.id/direktori/index/pengadilan/pn-surabaya/kategori/narkotika-dan-psikotropika-1/tahunjenis/putus/tahun/2025.html\")"
   ],
   "id": "29f7a72fc03479bd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping progress:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping with url: https://putusan3.mahkamahagung.go.id/direktori/index/pengadilan/pn-surabaya/kategori/narkotika-dan-psikotropika-1/tahunjenis/putus/tahun/2025.html - approx 640 data - 32 pages\n",
      "https://putusan3.mahkamahagung.go.id/direktori/index/pengadilan/pn-surabaya/kategori/narkotika-dan-psikotropika-1/tahunjenis/putus/tahun/2025.html?obf=TANGGAL_PUTUS&obm=desc\n",
      "https://putusan3.mahkamahagung.go.id/direktori/index/pengadilan/pn-surabaya/kategori/narkotika-dan-psikotropika-1/tahunjenis/putus/tahun/2025.html?page=2&obf=TANGGAL_PUTUS&obm=desc\n",
      "https://putusan3.mahkamahagung.go.id/direktori/index/pengadilan/pn-surabaya/kategori/narkotika-dan-psikotropika-1/tahunjenis/putus/tahun/2025.html?page=3&obf=TANGGAL_PUTUS&obm=desc\n",
      "https://putusan3.mahkamahagung.go.id/direktori/index/pengadilan/pn-surabaya/kategori/narkotika-dan-psikotropika-1/tahunjenis/putus/tahun/2025.html?page=4&obf=TANGGAL_PUTUS&obm=desc\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
